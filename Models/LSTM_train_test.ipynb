{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ9ae0g7ISl2"
      },
      "source": [
        "### **Long Short Term Memory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr8jwTVRjAsg"
      },
      "outputs": [],
      "source": [
        "#Initially we download the zipped data\n",
        "\n",
        "import os \n",
        "if not os.path.isfile('data.zip'):\n",
        "    !wget https://github.com/LeonardoBerti07/Deep-Learning-Algorithms-for-financial-time-serie-modeling-/blob/main/Dataset/data.zip\n",
        "    print('data downloaded.')\n",
        "else:\n",
        "    print('data already existed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqqAFVGl8ogy",
        "outputId": "5d5a82d4-f849-44a5-c27c-db720da4fa24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Load packages\n",
        "from faulthandler import dump_traceback\n",
        "import time\n",
        "import datetime\n",
        "from unicodedata import name\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms  \n",
        "from torch import nn\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-4cB_DIJAS5"
      },
      "source": [
        "### **Data**\n",
        "The dataset in the folder Dataset is the LOBSTER dataset zipped and normalized. I have combined the data of the 5 stocks available for free. I used the version with 10 levels, so we have 40 columns, in fact for every level we have a quadruple wiht the ask and bid prices and with the volumes associated, for more information i reference to the thesis. \n",
        "\n",
        "I used 70% to do the training, 15% to do the validation and 15% for the testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRk_lVmatWju"
      },
      "outputs": [],
      "source": [
        "# please change the data_path to your local path\n",
        "\n",
        "num_classes = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data_path =  \"/data.npy\"\n",
        "\n",
        "dec = np.load(data_path)\n",
        "\n",
        "train_size = int(0.70 * dec.shape[0])\n",
        "val_size = int(0.15 * dec.shape[0])\n",
        "test_size = val_size\n",
        "\n",
        "dec_train = dec[:train_size]\n",
        "dec_val = dec[train_size:val_size+train_size]\n",
        "dec_test = dec[val_size+train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Pbd5CqbVDMQ"
      },
      "outputs": [],
      "source": [
        "T = 50   #horizon\n",
        "\n",
        "def labeling(X, T):\n",
        "\n",
        "  [N, D] = X.shape\n",
        "  print(N)\n",
        "  Y = np.zeros((X.shape[0] - 2*T + 1))\n",
        "  alpha = 0.00072\n",
        "  media = []\n",
        "  for i in range(0, X.shape[0]- 2*T + 1):\n",
        "    ask_minus = X[i:i+T, :1]\n",
        "    bid_minus = X[i:i+T, 2:3]\n",
        "    ask_plus = X[i+T:i+2*T, :1]\n",
        "    bid_plus = X[i+T:i+2*T, 2:3]\n",
        "    m_minus = (ask_minus + bid_minus) / 2\n",
        "    m_minus = np.sum(m_minus) / T\n",
        "    m_plus = (ask_plus + bid_plus) / 2\n",
        "    m_plus = np.sum(m_plus) / T\n",
        "    media.append((m_plus - m_minus) / m_minus)\n",
        "    if (m_plus - m_minus) / m_minus < -alpha:\n",
        "      label = 1\n",
        "    elif (m_plus - m_minus) / m_minus > alpha:\n",
        "      label = 0\n",
        "    else:\n",
        "      label = 2\n",
        "    Y[i] = label\n",
        "  \n",
        "  plt.hist(Y)\n",
        "  plt.show()\n",
        "\n",
        "  return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G8Bti2mWqbs"
      },
      "outputs": [],
      "source": [
        "#Create the Dataset\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
        "    def __init__(self, x, y, num_classes, T):\n",
        "        \"\"\"Initialization\"\"\" \n",
        "        self.num_classes = num_classes\n",
        "        self.T = T\n",
        "        self.x = x   \n",
        "        self.y = y\n",
        "       \n",
        "        self.length = x.shape[0] - 2*T + 1\n",
        "        \n",
        "        x = torch.from_numpy(x)\n",
        "        self.x = torch.unsqueeze(x, 1)\n",
        "        self.y = torch.from_numpy(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the total number of samples\"\"\"\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        input = self.x[i:i+self.T, :]\n",
        "        input = input.permute(1, 0, 2)\n",
        "        #print(input.shape)\n",
        "        input = input.reshape((input.shape[1], input.shape[2]))\n",
        "        #print(input.shape)\n",
        "        return dict(sequence = input.float(), label = self.y[i].float())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXjmkkbcEleF"
      },
      "outputs": [],
      "source": [
        "#Create the DataLoader\n",
        "class PriceDataModule():\n",
        "    def __init__(self, train_sequences, val_sequences, test_sequences, num_workers=1, batch_size = 8):\n",
        "        super().__init__()\n",
        "        self.train_sequences = train_sequences\n",
        "        self.test_sequences = test_sequences\n",
        "        self.val_sequences = val_sequences\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def setup(self, stage=None):        \n",
        "        self.train_dataset = Dataset(dec_train, y_train, num_classes=3, T=50)\n",
        "        self.test_dataset = Dataset(dec_test, y_test, num_classes=3, T=50)\n",
        "        self.val_dataset = Dataset(dec_val, y_val, num_classes=3, T=50)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(dataset=self.train_dataset, batch_size=batch_size, shuffle=True, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(dataset=self.val_dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(dataset=self.test_dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "y_val = labeling(dec_val, T)\n",
        "y_test = labeling(dec_test, T)\n",
        "y_train = labeling(dec_train, T)\n",
        "\n",
        "data_module = PriceDataModule(dec_train, dec_val, dec_test, 2, batch_size)\n",
        "data_module.setup()\n",
        "train_dataloader = data_module.train_dataloader()\n",
        "test_dataloader = data_module.test_dataloader()\n",
        "val_dataloader = data_module.val_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-fSzRyhKptR"
      },
      "source": [
        "### **Model Architecture**\n",
        "The architecture has a single layer and a hidden size of 64. The reason for the simplicity of the architecture is that with a greater amount of layers and a greater hidden size the model tends to go in overfitting. As for the hyperparameters, the dropout is equal to 0.5 (to avoid overfitting as much as possible), the learning rate at 0.00005, the batch size is 64. The model has a total of 27,587 parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04vdrGfaEmdv"
      },
      "outputs": [],
      "source": [
        "#Create the LSTM model\n",
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, n_features, num_classes, dropout, n_hidden, n_layers):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "        input_size = n_features,\n",
        "        hidden_size = n_hidden,\n",
        "        batch_first = True,\n",
        "        num_layers = n_layers, # Stack LSTMs\n",
        "        dropout = dropout       \n",
        "    )\n",
        "        self.classifier = nn.Linear(n_hidden, num_classes) #after we have analyze the sequence, now we have to perform the classification, we utilize a classic linear layer\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.lstm.flatten_parameters()  \n",
        "        #x shape is (batch_size, seq_len, n_features)\n",
        "\n",
        "        output, (hidden, _) = self.lstm(x)\n",
        "        out = hidden[-1]  #We want the output from the last layer of the last time step to go into the final regressor linear layer, we don't use output \n",
        "                          #because in the output there are the H_t for each time step from the last layer, so the dim is (batch_size, L, Hidden_size)\n",
        "                          #instead in hidden there are hidden state for every layer but only for the last time step, so the dim is (num_layer, Hidden_size) \n",
        "                          #so if batch_size=1, the last array of output = hidden[-1]\n",
        "      \n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "epochs = 50\n",
        "lr = 0.00005\n",
        "dropout = 0.4\n",
        "\n",
        "\n",
        "    #input shape \n",
        "sequence_length = 50   #each sequence is composed by 5 day\n",
        "batch_size = 64\n",
        "num_features = 40\n",
        "\n",
        "    #hidden shape\n",
        "num_layer = 1\n",
        "hidden_size = 64\n",
        "\n",
        "model = myLSTM(num_features, num_classes, dropout, hidden_size, num_layer).to(device)\n",
        "model.float()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr) "
      ],
      "metadata": {
        "id": "4w0Oy2cw8GyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5LqLqdnK2Kx"
      },
      "source": [
        "### **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy_fnkqyErwc"
      },
      "outputs": [],
      "source": [
        "#Training the model\n",
        "def trainingLoop(train_dataloader, model, loss_fn, optimizer):\n",
        "    cont = 0\n",
        "    train_loss = 0\n",
        "    for batch in train_dataloader:         #scorriamo i vari batch del data set, in batch c'è l'index\n",
        "        labels = batch[\"label\"]\n",
        "        sequences = batch[\"sequence\"].to(device)\n",
        "\n",
        "        labels = labels.type(torch.LongTensor)\n",
        "        labels = labels.to(device)\n",
        "        #print(sequences.shape)\n",
        "        #forward pass\n",
        "        outputs = model(sequences)            #we do the prediction\n",
        "        loss = loss_fn(outputs, labels)     #compute the error\n",
        "        train_loss += loss.item()\n",
        "        #backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cont+=1\n",
        "\n",
        "    train_loss = train_loss / cont   #we compute the average train loss\n",
        "    print(\"trainining loss   ->   \" + str(train_loss)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvRcXX8eK_Wx"
      },
      "source": [
        "### **Model Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zblxhjOK-Yy"
      },
      "outputs": [],
      "source": [
        "#Validation\n",
        "def valLoop(test_dataloader, model, loss_fn, test_size):\n",
        "    true = [0, 0, 0]\n",
        "    denom = [0, 0, 0, 0, 0, 0]\n",
        "    num_batches = len(test_dataloader)\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "    cont = 0\n",
        "    with torch.no_grad():      \n",
        "        for batch in test_dataloader:         #we scroll through the various batches of the data set\n",
        "            \n",
        "            labels = batch[\"label\"]\n",
        "            sequences = batch[\"sequence\"].to(device)\n",
        "\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(sequences)            #we do the prediction\n",
        "            predicted = outputs.argmax(1)\n",
        "            \n",
        "            test_loss += loss_fn(outputs, labels).item()       \n",
        "            correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()     #we count the correct ones\n",
        "            cont += 1\n",
        "\n",
        "    print(\"validation loss   ->   \" + str(test_loss / cont))\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv8WFlRzWjTg"
      },
      "outputs": [],
      "source": [
        "print(\"------- List Hyper Parameters -------\")\n",
        "print(\"epochs   ->   \" + str(epochs))\n",
        "print(\"learningRate   ->   \" + str(lr))\n",
        "print(\"dropout   ->   \" + str(dropout))\n",
        "print(\"training range   ->   \" + str(train_size))\n",
        "print(\"number of layer   ->    \" + str(num_layer))\n",
        "print(\"sequence length    ->     \" + str(sequence_length))\n",
        "print(\"hidden size    ->    \" + str(hidden_size))\n",
        "\n",
        "best_test_loss = 99999\n",
        "best_val_loss = 99999\n",
        "for e in range(epochs):       \n",
        "    print(\"------------Start of Epoch {}/{}------------\".format(e, epochs-1))\n",
        "\n",
        "    #training\n",
        "    trainingLoop(train_dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "    #validation\n",
        "    val_loss = valLoop(val_dataloader, model, loss_fn, val_size)\n",
        "    if (val_loss < best_test_loss):   #we save the best model\n",
        "      torch.save(model, '/best_model_LSTM')\n",
        "      best_test_loss = val_loss\n",
        "      best_test_epoch = e\n",
        "      print('model saved')      \n",
        "    print(\"------------End of Epoch {}/{}------------\".format(e, epochs-1))\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Testing**"
      ],
      "metadata": {
        "id": "F6jhza6Y80VC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uymoqALXcOg"
      },
      "outputs": [],
      "source": [
        "#Testing \n",
        "def testLoop(test_dataloader, model, loss_fn, test_size):\n",
        "    true = [0, 0, 0]\n",
        "    denom = [0, 0, 0, 0, 0, 0]\n",
        "    num_batches = len(test_dataloader)\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    cont = 0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():      \n",
        "        for batch in test_dataloader:         #we scroll through the various batches of the data set\n",
        "            \n",
        "            labels = batch[\"label\"]\n",
        "            sequences = batch[\"sequence\"].to(device)\n",
        "\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(sequences)            #we do the prediction\n",
        "            predicted = outputs.argmax(1)\n",
        "            \n",
        "            test_loss += loss_fn(outputs, labels).item()       \n",
        "            correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()     #we count the correct ones\n",
        "            total += labels.shape[0]\n",
        "            \n",
        "            cont += 1\n",
        "            all_targets.append(labels.cpu().numpy())\n",
        "            all_predictions.append(predicted.cpu().numpy())\n",
        "    all_targets = np.concatenate(all_targets)    \n",
        "    all_predictions = np.concatenate(all_predictions)  \n",
        "    print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
        "    print(classification_report(all_targets, all_predictions, digits=4))\n",
        "    b \n",
        "\n",
        "    return test_loss / cont"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf-ZryxgUj2Y"
      },
      "outputs": [],
      "source": [
        "model = torch.load('/best_model_LSTM')\n",
        "\n",
        "#final test\n",
        "testLoop(test_dataloader, model, loss_fn, test_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
