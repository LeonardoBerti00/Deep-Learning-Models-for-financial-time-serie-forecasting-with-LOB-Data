{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Long Short Term Memory**"
      ],
      "metadata": {
        "id": "LZ9ae0g7ISl2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCKCWKyQGvD_"
      },
      "outputs": [],
      "source": [
        "#Initially we download the zipped data\n",
        "\n",
        "import os \n",
        "if not os.path.isfile('data.zip'):\n",
        "    !wget https://github.com/LeonardoBerti07/Deep-Learning-Algorithms-for-financial-time-serie-modeling-/blob/main/Datasets/DB2.zip\n",
        "    !unzip -n data.zip\n",
        "    print('data downloaded.')\n",
        "else:\n",
        "    print('data already existed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqqAFVGl8ogy",
        "outputId": "96067af7-27d7-4d1c-9de1-b3c2c9e502bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Load packages\n",
        "from faulthandler import dump_traceback\n",
        "import time\n",
        "import datetime\n",
        "from unicodedata import name\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms  \n",
        "from torch import nn\n",
        "from google.colab import drive\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data**\n",
        "The dataset in the folder Dataset is the LOBSTER dataset zipped and normalized. I have combined the data of the 5 stocks available for free. I used the version with 10 levels, so we have 40 columns, in fact for every level we have a quadruple wiht the ask and bid prices and with the volumes associated, for more information i reference to the thesis. \n",
        "\n",
        "I used 70% to do the training, 15% to do the validation and 15% for the testing."
      ],
      "metadata": {
        "id": "U-4cB_DIJAS5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRk_lVmatWju"
      },
      "outputs": [],
      "source": [
        "# please change the data_path to your local path\n",
        "\n",
        "num_classes = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data_path = \"/DB2.npy\"\n",
        "\n",
        "\n",
        "dec = np.load(data_path)\n",
        "\n",
        "train_size = int(0.70 * dec.shape[0])\n",
        "val_size = int(0.15 * dec.shape[0])\n",
        "test_size = val_size\n",
        "\n",
        "dec_train = dec[:train_size]\n",
        "dec_val = dec[train_size:val_size+train_size]\n",
        "dec_test = dec[val_size+train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "8Pbd5CqbVDMQ",
        "outputId": "3f74a2f6-cbd3-475b-e485-22d82b29f494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1477602, 40)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(316629, 40)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARg0lEQVR4nO3df4xldXnH8fenrL9/sbgrJUBdbDcxYKriBqk1LUoDC8YuptZA2rJa6mrFRtOmKdakGK0p/tHakLY0VDcujRUpaqUVilugMa1ZZLDIDxUZEctuEFYWQWOqxT79435XD+P9zszuzL0zsu9XcnPPfc73nPPMuXfvZ+45Z+6mqpAkaZyfWukGJEmrlyEhSeoyJCRJXYaEJKnLkJAkda1Z6QaW27p162rDhg0r3YYk/US5+eabv1lV6+fWH3chsWHDBmZmZla6DUn6iZLk6+PqHm6SJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1Pe7+4lqSVtKGCz61Ytu+56JXLfs6/SQhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKlrwZBIcmySG5J8MckdSd7W6kck2Znkrna/ttWT5OIks0luTXLiYF1b2/i7kmwd1F+S5La2zMVJMt82JEnTsZhPEo8Cf1BVxwMnA+cnOR64ALiuqjYC17XHAGcAG9ttG3AJjN7wgQuBlwInARcO3vQvAd44WG5zq/e2IUmaggVDoqruq6rPt+lvA18Cjga2ADvasB3AWW16C3BZjewCDk9yFHA6sLOq9lXVQ8BOYHOb98yq2lVVBVw2Z13jtiFJmoIDOieRZAPwYuBG4Miquq/N+gZwZJs+Grh3sNjuVpuvvntMnXm2MbevbUlmkszs3bv3QH4kSdI8Fh0SSZ4OfAx4e1U9MpzXPgHUMvf2GPNto6ourapNVbVp/fr1k2xDkg4piwqJJE9gFBAfrqqPt/L97VAR7f6BVt8DHDtY/JhWm69+zJj6fNuQJE3BYq5uCvBB4EtV9ReDWVcB+69Q2gp8clA/t13ldDLwcDtkdC1wWpK17YT1acC1bd4jSU5u2zp3zrrGbUOSNAVrFjHmF4HfAm5Lckur/TFwEXBFkvOArwOva/OuBs4EZoHvAm8AqKp9Sd4D3NTGvbuq9rXptwAfAp4CXNNuzLMNSdIULBgSVfUfQDqzTx0zvoDzO+vaDmwfU58BXjCm/uC4bUiSpsO/uJYkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktS1YEgk2Z7kgSS3D2rvSrInyS3tduZg3juSzCa5M8npg/rmVptNcsGgflySG1v9o0me2OpPao9n2/wNy/VDS5IWZzGfJD4EbB5Tf39VvajdrgZIcjxwNnBCW+ZvkhyW5DDgr4EzgOOBc9pYgPe1df0c8BBwXqufBzzU6u9v4yRJU7RgSFTVZ4B9i1zfFuDyqvpeVX0NmAVOarfZqrq7qr4PXA5sSRLglcCVbfkdwFmDde1o01cCp7bxkqQpWco5ibcmubUdjlrbakcD9w7G7G61Xv3ZwLeq6tE59cesq81/uI3/MUm2JZlJMrN3794l/EiSpKGDDYlLgJ8FXgTcB/z5snV0EKrq0qraVFWb1q9fv5KtSNLjykGFRFXdX1U/qKr/A/6O0eEkgD3AsYOhx7Rar/4gcHiSNXPqj1lXm/+sNl6SNCUHFRJJjho8fA2w/8qnq4Cz25VJxwEbgc8BNwEb25VMT2R0cvuqqirgBuC1bfmtwCcH69rapl8LXN/GS5KmZM1CA5J8BDgFWJdkN3AhcEqSFwEF3AO8CaCq7khyBfBF4FHg/Kr6QVvPW4FrgcOA7VV1R9vEHwGXJ/lT4L+AD7b6B4G/TzLL6MT52Uv+aSVJB2TBkKiqc8aUPzimtn/8e4H3jqlfDVw9pn43PzpcNaz/D/DrC/UnSZoc/+JaktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlS14IhkWR7kgeS3D6oHZFkZ5K72v3aVk+Si5PMJrk1yYmDZba28Xcl2TqovyTJbW2Zi5Nkvm1IkqZnMZ8kPgRsnlO7ALiuqjYC17XHAGcAG9ttG3AJjN7wgQuBlwInARcO3vQvAd44WG7zAtuQJE3JgiFRVZ8B9s0pbwF2tOkdwFmD+mU1sgs4PMlRwOnAzqraV1UPATuBzW3eM6tqV1UVcNmcdY3bhiRpSg72nMSRVXVfm/4GcGSbPhq4dzBud6vNV989pj7fNn5Mkm1JZpLM7N279yB+HEnSOEs+cd0+AdQy9HLQ26iqS6tqU1VtWr9+/SRbkaRDysGGxP3tUBHt/oFW3wMcOxh3TKvNVz9mTH2+bUiSpuRgQ+IqYP8VSluBTw7q57arnE4GHm6HjK4FTkuytp2wPg24ts17JMnJ7aqmc+esa9w2JElTsmahAUk+ApwCrEuym9FVShcBVyQ5D/g68Lo2/GrgTGAW+C7wBoCq2pfkPcBNbdy7q2r/yfC3MLqC6inANe3GPNuQJE3JgiFRVed0Zp06ZmwB53fWsx3YPqY+A7xgTP3BcduQJE2Pf3EtSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpa0khkeSeJLcluSXJTKsdkWRnkrva/dpWT5KLk8wmuTXJiYP1bG3j70qydVB/SVv/bFs2S+lXknRgluOTxCuq6kVVtak9vgC4rqo2Ate1xwBnABvbbRtwCYxCBbgQeClwEnDh/mBpY944WG7zMvQrSVqkSRxu2gLsaNM7gLMG9ctqZBdweJKjgNOBnVW1r6oeAnYCm9u8Z1bVrqoq4LLBuiRJU7DUkCjg00luTrKt1Y6sqvva9DeAI9v00cC9g2V3t9p89d1j6j8mybYkM0lm9u7du5SfR5I0sGaJy7+8qvYkeQ6wM8mXhzOrqpLUErexoKq6FLgUYNOmTRPfniQdKpb0SaKq9rT7B4BPMDqncH87VES7f6AN3wMcO1j8mFabr37MmLokaUoOOiSSPC3JM/ZPA6cBtwNXAfuvUNoKfLJNXwWc265yOhl4uB2WuhY4LcnadsL6NODaNu+RJCe3q5rOHaxLkjQFSzncdCTwiXZV6hrgH6rqX5PcBFyR5Dzg68Dr2virgTOBWeC7wBsAqmpfkvcAN7Vx766qfW36LcCHgKcA17SbJGlKDjokqupu4IVj6g8Cp46pF3B+Z13bge1j6jPACw62R0nS0vgX15KkrqVe3fS4suGCT63Ytu+56FUrtm1J6vGThCSpy5CQJHV5uEmaEg9n6ieRnyQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSulZ9SCTZnOTOJLNJLljpfiTpULKqQyLJYcBfA2cAxwPnJDl+ZbuSpEPHqg4J4CRgtqrurqrvA5cDW1a4J0k6ZKxZ6QYWcDRw7+DxbuClcwcl2QZsaw+/k+TOg9zeOuCbB7nskuR9885esb4WYF8HxtfXgbGvA5T3Lam3544rrvaQWJSquhS4dKnrSTJTVZuWoaVlZV8Hxr4OjH0dmNXaF0ymt9V+uGkPcOzg8TGtJkmagtUeEjcBG5Mcl+SJwNnAVSvckyQdMlb14aaqejTJW4FrgcOA7VV1xwQ3ueRDVhNiXwfGvg6MfR2Y1doXTKC3VNVyr1OS9Dix2g83SZJWkCEhSeo6ZEJioa/3SPKkJB9t829MsmEw7x2tfmeS06fc1+8n+WKSW5Ncl+S5g3k/SHJLuy3rCf1F9PX6JHsH2/+dwbytSe5qt61T7uv9g56+kuRbg3kT2V9Jtid5IMntnflJcnHr+dYkJw7mTXJfLdTXb7R+bkvy2SQvHMy7p9VvSTIz5b5OSfLw4Ln6k8G8iX1NzyL6+sNBT7e319MRbd4k99exSW5o7wN3JHnbmDGTe41V1eP+xuik91eB5wFPBL4AHD9nzFuAv23TZwMfbdPHt/FPAo5r6zlsin29Anhqm/7d/X21x99Zwf31euCvxix7BHB3u1/bptdOq68543+P0cUOk95fvwScCNzemX8mcA0Q4GTgxknvq0X29bL922P01Tc3DubdA6xbof11CvAvS33+l7uvOWNfDVw/pf11FHBim34G8JUx/x4n9ho7VD5JLObrPbYAO9r0lcCpSdLql1fV96rqa8BsW99U+qqqG6rqu+3hLkZ/KzJpS/k6lNOBnVW1r6oeAnYCm1eor3OAjyzTtruq6jPAvnmGbAEuq5FdwOFJjmKy+2rBvqrqs227ML3X1mL2V89Ev6bnAPuaymsLoKruq6rPt+lvA19i9G0UQxN7jR0qITHu6z3m7uQfjqmqR4GHgWcvctlJ9jV0HqPfFvZ7cpKZJLuSnLVMPR1IX7/WPtpemWT/Hz2uiv3VDssdB1w/KE9qfy2k1/ck99WBmvvaKuDTSW7O6Gtvpu0XknwhyTVJTmi1VbG/kjyV0RvtxwblqeyvjA6Dvxi4cc6sib3GVvXfSehHkvwmsAn45UH5uVW1J8nzgOuT3FZVX51SS/8MfKSqvpfkTYw+hb1ySttejLOBK6vqB4PaSu6vVSvJKxiFxMsH5Ze3ffUcYGeSL7fftKfh84yeq+8kORP4J2DjlLa9GK8G/rOqhp86Jr6/kjydUTC9vaoeWc51z+dQ+SSxmK/3+OGYJGuAZwEPLnLZSfZFkl8B3gn8alV9b3+9qva0+7uBf2f0G8ZU+qqqBwe9fAB4yWKXnWRfA2cz53DABPfXQnp9r/jXziT5eUbP35aqenB/fbCvHgA+wfIdYl1QVT1SVd9p01cDT0iyjlWwv5r5XlsT2V9JnsAoID5cVR8fM2Ryr7FJnGhZbTdGn5juZnT4Yf8JrxPmjDmfx564vqJNn8BjT1zfzfKduF5MXy9mdLJu45z6WuBJbXodcBfLdBJvkX0dNZh+DbCrfnSi7Gutv7Vt+ohp9dXGPZ/RicRMY3+1dW6gfyL2VTz2pOLnJr2vFtnXzzA6x/ayOfWnAc8YTH8W2DzFvn56/3PH6M32v9u+W9TzP6m+2vxnMTpv8bRp7a/2s18G/OU8Yyb2Glu2nbvab4zO/n+F0RvuO1vt3Yx+Owd4MvCP7R/N54DnDZZ9Z1vuTuCMKff1b8D9wC3tdlWrvwy4rf1DuQ04b8p9/RlwR9v+DcDzB8v+dtuPs8AbptlXe/wu4KI5y01sfzH6rfI+4H8ZHfM9D3gz8OY2P4z+86yvtm1vmtK+WqivDwAPDV5bM63+vLafvtCe43dOua+3Dl5buxiE2Ljnf1p9tTGvZ3Qhy3C5Se+vlzM653Hr4Lk6c1qvMb+WQ5LUdaick5AkHQRDQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnr/wE1qFVBincMqgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(316629, 40)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "\n",
        "#Hyperparameters\n",
        "epochs = 50\n",
        "lr = 0.00005\n",
        "regularization = 0.5\n",
        "\n",
        "    #input shape \n",
        "sequence_length = 50   #each sequence is composed by 5 day\n",
        "batch_size = 64\n",
        "num_features = 40\n",
        "\n",
        "    #hidden shape\n",
        "num_layer = 1\n",
        "hidden_size = 64\n",
        "\n",
        "#Create Sequences for the LSTM\n",
        "def create_sequences(input_data, seq_len):\n",
        "    sequences = []\n",
        "    data_size = input_data.shape[0]\n",
        "    Y = []\n",
        "    print(input_data.shape)\n",
        "    alpha = 0.0007226735755120588\n",
        "    for i in range(data_size - 2*seq_len):\n",
        "        sequence = input_data[i:i+seq_len, :]\n",
        "        label_position = i + seq_len             \n",
        "        ask_minus = sequence[:, :1]\n",
        "        bid_minus = sequence[:, 2:3]\n",
        "        ask_plus = input_data[(i+seq_len):(i+2*seq_len), :1]\n",
        "        bid_plus = input_data[(i+seq_len):i+2*seq_len, 2:3]\n",
        "        m_minus = (ask_minus + bid_minus) / 2\n",
        "        m_minus = np.sum(m_minus) / seq_len\n",
        "        m_plus = (ask_plus + bid_plus) / 2\n",
        "        m_plus = np.sum(m_plus) / seq_len\n",
        "        #print((m_plus - m_minus) / m_minus)\n",
        "        if (m_plus - m_minus) / m_minus < -alpha:\n",
        "          label = 1\n",
        "        elif (m_plus - m_minus) / m_minus > alpha:\n",
        "          label = 0\n",
        "        else:\n",
        "          label = 2          \n",
        "        Y.append(label)\n",
        "        sequences.append((sequence, label))\n",
        "        \n",
        "    plt.hist(Y)\n",
        "    plt.show()\n",
        "    return sequences\n",
        "\n",
        "train_sequences = create_sequences(dec_train, sequence_length)\n",
        "test_sequences = create_sequences(dec_test, sequence_length)\n",
        "val_sequences = create_sequences(dec_val, sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G8Bti2mWqbs"
      },
      "outputs": [],
      "source": [
        "#Create the Dataset\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, sequences):  \n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):    #return the len of the dataste\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):  #idx is the index of the sequence that we want to load\n",
        "        sequence, label = self.sequences[idx]\n",
        "\n",
        "        return dict(\n",
        "            sequence = torch.from_numpy(sequence).float(),\n",
        "            label = label\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXjmkkbcEleF"
      },
      "outputs": [],
      "source": [
        "#Create the DataLoader\n",
        "class PriceDataModule():\n",
        "    def __init__(self, train_sequences, val_sequences, test_sequences, num_workers=1, batch_size = 8):\n",
        "        super().__init__()\n",
        "        self.train_sequences = train_sequences\n",
        "        self.test_sequences = test_sequences\n",
        "        self.val_sequences = val_sequences\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def setup(self, stage=None):        \n",
        "        self.train_dataset = Dataset(self.train_sequences)\n",
        "        self.test_dataset = Dataset(self.test_sequences)\n",
        "        self.val_dataset = Dataset(self.val_sequences)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size = self.batch_size,\n",
        "            shuffle = True,\n",
        "            num_workers=self.num_workers\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size = self.batch_size,\n",
        "            shuffle = False,\n",
        "            num_workers=self.num_workers\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size = self.batch_size,\n",
        "            shuffle = False,\n",
        "            num_workers=self.num_workers\n",
        "        )\n",
        "\n",
        "data_module = PriceDataModule(train_sequences, val_sequences, test_sequences, 2, batch_size)\n",
        "data_module.setup()\n",
        "train_dataloader = data_module.train_dataloader()\n",
        "test_dataloader = data_module.test_dataloader()\n",
        "val_dataloader = data_module.val_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Architecture**\n",
        "The architecture has a single layer and a hidden size of 64. The reason for the simplicity of the architecture is that with a greater amount of layers and a greater hidden size the model tends to go in overfitting. As for the hyperparameters, the dropout is equal to 0.5 (to avoid overfitting as much as possible), the learning rate at 0.00005, the batch size is 64. The model has a total of 27,587 parameters."
      ],
      "metadata": {
        "id": "9-fSzRyhKptR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04vdrGfaEmdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f24802-d97c-4d99-8207-7ad274ad7de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ],
      "source": [
        "#Create the LSTM model\n",
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, n_features, num_classes, dropout, n_hidden, n_layers):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "        input_size = n_features,\n",
        "        hidden_size = n_hidden,\n",
        "        batch_first = True,\n",
        "        num_layers = n_layers, # Stack LSTMs\n",
        "        dropout = dropout       \n",
        "    )\n",
        "        self.classifier = nn.Linear(n_hidden, num_classes) #after we have analyze the sequence, now we have to perform the classification, we utilize a classic linear layer\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.lstm.flatten_parameters()  \n",
        "        #x shape is (batch_size, seq_len, n_features)\n",
        "\n",
        "        output, (hidden, _) = self.lstm(x)\n",
        "        out = hidden[-1]  #We want the output from the last layer of the last time step to go into the final regressor linear layer, we don't use output \n",
        "                          #because in the output there are the H_t for each time step from the last layer, so the dim is (batch_size, L, Hidden_size)\n",
        "                          #instead in hidden there are hidden state for every layer but only for the last time step, so the dim is (num_layer, Hidden_size) \n",
        "                          #so if batch_size=1, the last array of output = hidden[-1]\n",
        "      \n",
        "        return self.classifier(out)\n",
        "\n",
        "model = myLSTM(num_features, num_classes, regularization, hidden_size, num_layer).to(device)\n",
        "model.float()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Training**"
      ],
      "metadata": {
        "id": "K5LqLqdnK2Kx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy_fnkqyErwc"
      },
      "outputs": [],
      "source": [
        "#Training the model\n",
        "def trainingLoop(train_dataloader, model, loss_fn, optimizer):\n",
        "    cont = 0\n",
        "    train_loss = 0\n",
        "    for batch in train_dataloader:         #scorriamo i vari batch del data set, in batch c'è l'index\n",
        "        sequences = batch[\"sequence\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        #forward pass\n",
        "        outputs = model(sequences)            #we do the prediction\n",
        "        loss = loss_fn(outputs, labels)     #compute the error\n",
        "        train_loss += loss\n",
        "        #backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cont+=1\n",
        "\n",
        "    train_loss = train_loss / cont   #we compute the average train loss\n",
        "    print()\n",
        "    print(train_loss) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Testing**"
      ],
      "metadata": {
        "id": "JvRcXX8eK_Wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing \n",
        "def testLoop(test_dataloader, model, loss_fn, test_size):\n",
        "    true = [0, 0, 0]\n",
        "    denom = [0, 0, 0, 0, 0, 0]\n",
        "    num_batches = len(test_dataloader)\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():      \n",
        "        for batch in test_dataloader:         #we scroll through the various batches of the data set\n",
        "            sequences = batch[\"sequence\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "   \n",
        "            outputs = model(sequences)            #we do the prediction\n",
        "            predicted = outputs.argmax(1)\n",
        "            \n",
        "            test_loss += loss_fn(outputs, labels).item()       \n",
        "            correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()     #we count the correct ones\n",
        "            \n",
        "            for i in range(labels.size(dim=0)):   #compute the data to calculate precision and recall\n",
        "              if (labels[i] == predicted[i] == 0):\n",
        "                true[0] += 1\n",
        "                denom[0] += 1\n",
        "                denom[1] += 1\n",
        "              elif (labels[i] == predicted[i] == 1):\n",
        "                true[1] += 1\n",
        "                denom[2] += 1\n",
        "                denom[3] += 1\n",
        "              elif (labels[i] == predicted[i] == 2):\n",
        "                true[2] += 1\n",
        "                denom[4] += 1\n",
        "                denom[5] += 1\n",
        "              elif (labels[i] == 0 and predicted[i] == 1):\n",
        "                denom[1] += 1\n",
        "                denom[2] += 1\n",
        "              elif (labels[i] == 0 and predicted[i] == 2):\n",
        "                denom[1] += 1\n",
        "                denom[4] += 1\n",
        "              elif (labels[i] == 1 and predicted[i] == 0):\n",
        "                denom[3] += 1\n",
        "                denom[0] += 1\n",
        "              elif (labels[i] == 1 and predicted[i] == 2):\n",
        "                denom[3] += 1\n",
        "                denom[4] += 1\n",
        "              elif (labels[i] == 2 and predicted[i] == 0):\n",
        "                denom[5] += 1\n",
        "                denom[0] += 1\n",
        "              elif (labels[i] == 2 and predicted[i] == 1):\n",
        "                denom[5] += 1\n",
        "                denom[2] += 1\n",
        "\n",
        "        test_loss = test_loss / num_batches        #average loss \n",
        "        correct = correct / test_size\n",
        "        if (denom[1] != 0):\n",
        "          recallUp = true[0] / denom[1]\n",
        "        else:\n",
        "          recallUp = 0\n",
        "\n",
        "        if (denom[3] != 0):\n",
        "          recallDown = true[1] / denom[3]\n",
        "        else:\n",
        "          recallDown = 0  \n",
        "\n",
        "        if (denom[5] != 0):\n",
        "          recallNo = true[2] / denom[5]\n",
        "        else:\n",
        "          recallNo = 0\n",
        "\n",
        "        if (denom[0] != 0):\n",
        "          precisionUp = true[0] / denom[0]\n",
        "        else:\n",
        "          precisionUp = 0\n",
        "\n",
        "        if (denom[2] != 0):\n",
        "          precisionDown = true[1] / denom[2]\n",
        "        else:\n",
        "          precisionDown = 0\n",
        "\n",
        "        if (denom[4] != 0):\n",
        "          precisionNo = true[2] / denom[4]\n",
        "        else:\n",
        "          precisionNo = 0\n",
        "\n",
        "        print(f\"Accuracy: {correct * 100}, Average loss: {test_loss}\")\n",
        "        print(f\"Precision Up: {precisionUp * 100}, Recall Up: {recallUp* 100}\")\n",
        "        print(f\"Precision Down: {precisionDown* 100}, Recall Down: {recallDown* 100}\")\n",
        "        print(f\"Precision No: {precisionNo* 100}, Recall No: {recallNo* 100}\")\n",
        "    return test_loss\n"
      ],
      "metadata": {
        "id": "6zblxhjOK-Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv8WFlRzWjTg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77863d46-8ae4-4a36-e783-a9f95f3890a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- List Hyper Parameters -------\n",
            "epochs   ->   50\n",
            "learningRate   ->   5e-05\n",
            "dropout   ->   0.5\n",
            "training range   ->   1477602\n",
            "number of layer   ->    1\n",
            "sequence length    ->     50\n",
            "hidden size    ->    64\n",
            "------------Start of Epoch 0/49------------\n"
          ]
        }
      ],
      "source": [
        "print(\"------- List Hyper Parameters -------\")\n",
        "print(\"epochs   ->   \" + str(epochs))\n",
        "print(\"learningRate   ->   \" + str(lr))\n",
        "print(\"dropout   ->   \" + str(regularization))\n",
        "print(\"training range   ->   \" + str(train_size))\n",
        "print(\"number of layer   ->    \" + str(num_layer))\n",
        "print(\"sequence length    ->     \" + str(sequence_length))\n",
        "print(\"hidden size    ->    \" + str(hidden_size))\n",
        "\n",
        "\n",
        "best_test_loss = 99999\n",
        "best_val_loss = 99999\n",
        "for e in range(epochs):       \n",
        "    print(\"------------Start of Epoch {}/{}------------\".format(e, epochs-1))\n",
        "    #training\n",
        "    trainingLoop(train_dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "    #validation\n",
        "    val_loss = testLoop(val_dataloader, model, loss_fn, val_size)\n",
        "    if (val_loss < best_test_loss):   #we save the best model\n",
        "      torch.save(model, 'best_model_LSTM')\n",
        "      best_test_loss = val_loss\n",
        "      best_test_epoch = e\n",
        "      print('model saved')      \n",
        "    print(\"------------End of Epoch {}/{}------------\".format(e, epochs-1))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf-ZryxgUj2Y"
      },
      "outputs": [],
      "source": [
        "model = torch.load('/best_model_LSTM')\n",
        "\n",
        "#final test\n",
        "testLoop(test_dataloader, model, loss_fn, test_size)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
